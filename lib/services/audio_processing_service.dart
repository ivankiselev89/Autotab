import '../models/note.dart';
import 'pitch_detection.dart';
import 'note_segmentation.dart';
import 'tab_generator.dart';
import 'midi_generator.dart';

/// Orchestrates the audio processing pipeline from raw audio to transcription
class AudioProcessingService {
  final PitchDetectionService _pitchDetection = PitchDetectionService();
  final NoteSegmentationService _noteSegmentation = NoteSegmentationService();
  final TabGeneratorService _tabGenerator = TabGeneratorService();

  /// Process raw audio data and generate a complete transcription
  /// 
  /// [audioData] - Raw audio samples (normalized -1.0 to 1.0)
  /// [sampleRate] - Sample rate in Hz (e.g., 44100)
  /// [instrument] - Instrument type for filtering ('guitar', 'piano', etc.)
  /// 
  /// Returns a TranscriptionResult with notes, tabs, and text notation
  Future<TranscriptionResult> processAudio({
    required List<double> audioData,
    double sampleRate = 44100.0,
    String instrument = 'guitar',
  }) async {
    if (audioData.isEmpty) {
      throw ArgumentError('Audio data cannot be empty');
    }

    // Step 1: Segment audio into individual notes
    final notes = _noteSegmentation.segmentAudio(
      audioData,
      sampleRate: sampleRate,
    );

    // Step 2: Filter notes based on instrument range (optional)
    final filteredNotes = notes.where((note) {
      return _pitchDetection.isPitchInRange(instrument, note.frequency.toDouble());
    }).toList();

    // Use filtered notes if we have them, otherwise use all notes
    final finalNotes = filteredNotes.isNotEmpty ? filteredNotes : notes;

    // Step 3: Generate tab notation
    final tabNotation = _tabGenerator.generateTab(finalNotes);

    // Step 4: Generate text notation
    final textNotation = _tabGenerator.generateTextNotation(finalNotes);

    return TranscriptionResult(
      notes: finalNotes,
      tabNotation: tabNotation,
      textNotation: textNotation,
      instrumentType: instrument,
      sampleRate: sampleRate,
      duration: audioData.length / sampleRate,
    );
  }

  /// Process audio file from path (must be converted to raw samples first)
  /// This is a helper method that would work with actual audio file decoding
  Future<TranscriptionResult> processAudioFile({
    required String filePath,
    double sampleRate = 44100.0,
    String instrument = 'guitar',
  }) async {
    // In a real implementation, you would:
    // 1. Use an audio decoding library to load the file
    // 2. Convert to mono if stereo
    // 3. Normalize samples to -1.0 to 1.0 range
    // 4. Pass to processAudio()
    
    throw UnimplementedError(
      'Audio file decoding requires an audio library like flutter_ffmpeg or just_audio'
    );
  }

  /// Detect pitch in real-time from audio buffer
  /// Useful for live pitch detection during recording
  double detectRealtimePitch({
    required List<double> audioBuffer,
    int sampleRate = 44100,
  }) {
    return _pitchDetection.detectPitch(audioBuffer, sampleRate: sampleRate);
  }
}

/// Result of audio transcription processing
class TranscriptionResult {
  final List<Note> notes;
  final String tabNotation;
  final String textNotation;
  final String instrumentType;
  final double sampleRate;
  final double duration;

  TranscriptionResult({
    required this.notes,
    required this.tabNotation,
    required this.textNotation,
    required this.instrumentType,
    required this.sampleRate,
    required this.duration,
  });

  /// Generate a formatted string representation of the transcription
  String toFormattedString({int bpm = 120}) {
    final timestamp = DateTime.now();
    return '''
Transcription Report
====================
Generated: $timestamp
Duration: ${duration.toStringAsFixed(2)} seconds
Sample Rate: ${sampleRate.toStringAsFixed(0)} Hz
Instrument: $instrumentType
BPM: $bpm
Notes Detected: ${notes.length}

$textNotation

Guitar Tablature:
$tabNotation

---
Generated by Autotab Audio Processing Service
''';
  }

  /// Export as MIDI file
  Future<void> exportToMidi(String outputPath, {int bpm = 120}) async {
    await MidiGeneratorService.generateMidiFromNotes(
      notes,
      outputPath,
      bpm: bpm,
    );
  }
}
